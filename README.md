# Data Pipeline
## Objective
Create a pipeline that ingests, processes, and outputs data in a scalable manner

## Tools
Python

SQL

Apache Airflow

Docker

## Steps
1. Use Apache Airflow to schedule and manage the data pipelines. Showing how Airflow orchestates tasks like data extraction, transformation, and loading (ETL)
2. Build the pipeline in Python, pulling data from APIs or CSV files, transforming it (e.g. cleaning or normalizing the data), and storing it in a SQL database
3. Use Docker to containerize the entire pipeline, ensuring it can be easily deployed and scaled in different environments

## Skill Showcased
- Building and managing data pipelines using standard tools 
- Writing reusable, scalable code in Python
- Automating workflows with Airflow, including failure handling
- Containerizing applications with Docker for scalable deployments
